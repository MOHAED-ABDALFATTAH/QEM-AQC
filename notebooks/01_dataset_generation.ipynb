{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import qiskit\n",
    "from itertools import product\n",
    "\n",
    "from qiskit import qasm2, QuantumCircuit\n",
    "from qiskit.quantum_info import Statevector, DensityMatrix\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_aer.noise import (\n",
    "    NoiseModel, \n",
    "    depolarizing_error, \n",
    "    amplitude_damping_error,\n",
    "    ReadoutError\n",
    ")\n",
    "from qiskit.primitives import StatevectorEstimator as Estimator\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.circuit.library import EfficientSU2 \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bf406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator:\n",
    "    def __init__(self, num_qubits=4, seed=42):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def create_random_circuit(self, depth=2, entanglement=\"linear\", circuit_id=0):\n",
    "        entanglement_map = {\n",
    "            'linear': 'linear',\n",
    "            'full': 'full',\n",
    "            'pairwise': 'pairwise'\n",
    "        }\n",
    "        \n",
    "        ent_pattern = entanglement_map.get(entanglement, 'linear')\n",
    "        \n",
    "        ansatz = EfficientSU2(\n",
    "            num_qubits=self.num_qubits,\n",
    "            reps=depth,\n",
    "            entanglement=ent_pattern,\n",
    "            insert_barriers=False\n",
    "        )\n",
    "        \n",
    "        num_params = ansatz.num_parameters\n",
    "        random_params = np.random.uniform(0, 2*np.pi, num_params)\n",
    "        \n",
    "        qc = ansatz.assign_parameters(random_params)\n",
    "        \n",
    "        qc.metadata = {\n",
    "            'circuit_id': circuit_id,\n",
    "            'num_qubits': self.num_qubits,\n",
    "            'depth': depth,\n",
    "            'entanglement': entanglement,\n",
    "            'ansatz_type': 'EfficientSU2',\n",
    "            'num_parameters': num_params,\n",
    "            'parameters': random_params.tolist()\n",
    "        }\n",
    "        \n",
    "        return qc\n",
    "    \n",
    "    def get_observables(self):\n",
    "        pauli_string = 'Z' * self.num_qubits\n",
    "        obs = SparsePauliOp(pauli_string)\n",
    "        return [(obs, f'Z_all')]\n",
    "    \n",
    "    def simulate_ideal(self, circuit):\n",
    "        sv = Statevector.from_instruction(circuit)\n",
    "        observables = self.get_observables()\n",
    "        expectations = {}\n",
    "        \n",
    "        for obs, label in observables:\n",
    "            exp_val = sv.expectation_value(obs).real\n",
    "            expectations[label] = exp_val\n",
    "        \n",
    "        return expectations\n",
    "    \n",
    "    def generate_dataset(self, num_circuits=20, base_depth=2, depth_multipliers=[1, 2, 3], \n",
    "                        entanglement_types=[\"linear\", \"full\", \"pairwise\"], output_dir='data/ideal'):\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        total_configs = len(depth_multipliers) * len(entanglement_types)\n",
    "        total_circuits = num_circuits * total_configs\n",
    "        \n",
    "        print(f\"Generating {total_circuits} circuits using EfficientSU2 ansatz\")\n",
    "        \n",
    "        dataset = []\n",
    "        circuit_counter = 0\n",
    "        \n",
    "        for depth_mult, entanglement in product(depth_multipliers, entanglement_types):\n",
    "            actual_depth = base_depth * depth_mult\n",
    "            \n",
    "            for i in range(num_circuits):\n",
    "                circuit = self.create_random_circuit(\n",
    "                    depth=actual_depth, \n",
    "                    entanglement=entanglement, \n",
    "                    circuit_id=circuit_counter\n",
    "                )\n",
    "                \n",
    "                ideal_expectations = self.simulate_ideal(circuit)\n",
    "                \n",
    "                ideal_expectations_serializable = {\n",
    "                    key: float(value) if hasattr(value, 'item') else value\n",
    "                    for key, value in ideal_expectations.items()\n",
    "                }\n",
    "                \n",
    "                qasm_str = qasm2.dumps(circuit)\n",
    "                \n",
    "                entry = {\n",
    "                    'circuit_id': int(circuit_counter),\n",
    "                    'num_qubits': int(self.num_qubits),\n",
    "                    'depth': int(actual_depth),\n",
    "                    'depth_multiplier': int(depth_mult),\n",
    "                    'base_depth': int(base_depth),\n",
    "                    'entanglement': entanglement,\n",
    "                    'ansatz_type': 'EfficientSU2',\n",
    "                    'num_parameters': int(circuit.metadata['num_parameters']),\n",
    "                    'ideal_expectations': ideal_expectations_serializable,\n",
    "                    'circuit_qasm': qasm_str\n",
    "                }\n",
    "                \n",
    "                dataset.append(entry)\n",
    "                \n",
    "                filename = f'circuit_{circuit_counter:04d}_d{actual_depth}_{entanglement}_ideal.json'\n",
    "                with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "                    json.dump(entry, f, indent=2)\n",
    "                \n",
    "                circuit_counter += 1\n",
    "                \n",
    "                if circuit_counter % 10 == 0:\n",
    "                    print(f\"Generated {circuit_counter}/{total_circuits} circuits\")\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372779dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseInjector:\n",
    "    def __init__(self, num_qubits=4):\n",
    "        self.num_qubits = num_qubits\n",
    "    \n",
    "    def create_noise_model(self, noise_type='depolarizing', error_rate=0.01):\n",
    "        noise_model = NoiseModel()\n",
    "        \n",
    "        if noise_type == 'depolarizing':\n",
    "            error = depolarizing_error(error_rate, 1)\n",
    "            noise_model.add_all_qubit_quantum_error(error, ['rx', 'ry', 'rz'])\n",
    "            \n",
    "            error_2q = depolarizing_error(min(error_rate * 10, 0.3), 2)\n",
    "            noise_model.add_all_qubit_quantum_error(error_2q, ['cx'])\n",
    "        \n",
    "        elif noise_type == 'amplitude_damping':\n",
    "            error = amplitude_damping_error(error_rate)\n",
    "            noise_model.add_all_qubit_quantum_error(error, ['rx', 'ry', 'rz'])\n",
    "        \n",
    "        elif noise_type == 'readout':\n",
    "            readout_error = ReadoutError([[1 - error_rate, error_rate],\n",
    "                                           [error_rate, 1 - error_rate]])\n",
    "            for qubit in range(self.num_qubits):\n",
    "                noise_model.add_readout_error(readout_error, [qubit])\n",
    "        \n",
    "        return noise_model\n",
    "    \n",
    "    def simulate_noisy(self, circuit, noise_model, shots=8192):\n",
    "        simulator = AerSimulator(noise_model=noise_model, method='density_matrix')\n",
    "        \n",
    "        pauli_string = 'Z' * self.num_qubits\n",
    "        observable = SparsePauliOp(pauli_string)\n",
    "        label = f'Z_all'\n",
    "        \n",
    "        qc_measure = circuit.copy()\n",
    "        qc_measure.save_density_matrix()\n",
    "        \n",
    "        result = simulator.run(qc_measure, shots=1).result()\n",
    "        noisy_dm = result.data()['density_matrix']\n",
    "        \n",
    "        exp_val = noisy_dm.expectation_value(observable).real\n",
    "        \n",
    "        return {label: float(exp_val)}\n",
    "    \n",
    "    def add_noise_to_dataset(self, ideal_dir='data/ideal', \n",
    "                            noisy_dir='data/noisy',\n",
    "                            noise_types=['depolarizing', 'amplitude_damping', 'readout'],\n",
    "                            error_rates=[0.001, 0.01, 0.1]):\n",
    "        Path(noisy_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        ideal_files = sorted(Path(ideal_dir).glob('circuit_*_ideal.json'))\n",
    "        noise_configs = list(product(noise_types, error_rates))\n",
    "        \n",
    "        print(f\"Applying noise to {len(ideal_files)} circuits...\")\n",
    "        \n",
    "        for config_idx, (noise_type, error_rate) in enumerate(noise_configs):\n",
    "            print(f\"Config {config_idx+1}/{len(noise_configs)}: {noise_type} (rate={error_rate})\")\n",
    "            \n",
    "            noise_model = self.create_noise_model(noise_type, error_rate)\n",
    "            \n",
    "            for ideal_file in ideal_files:\n",
    "                with open(ideal_file, 'r') as f:\n",
    "                    ideal_data = json.load(f)\n",
    "                \n",
    "                circuit_id = ideal_data['circuit_id']\n",
    "                qc = QuantumCircuit.from_qasm_str(ideal_data['circuit_qasm'])\n",
    "                noisy_expectations = self.simulate_noisy(qc, noise_model)\n",
    "                \n",
    "                noisy_entry = {\n",
    "                    'circuit_id': int(circuit_id),\n",
    "                    'num_qubits': int(ideal_data['num_qubits']),\n",
    "                    'depth': int(ideal_data['depth']),\n",
    "                    'depth_multiplier': int(ideal_data.get('depth_multiplier', 1)),\n",
    "                    'entanglement': ideal_data.get('entanglement', 'linear'),\n",
    "                    'ansatz_type': ideal_data.get('ansatz_type', 'EfficientSU2'),\n",
    "                    'noise_type': noise_type,\n",
    "                    'error_rate': float(error_rate),\n",
    "                    'noisy_expectations': noisy_expectations,\n",
    "                    'ideal_expectations': ideal_data['ideal_expectations']\n",
    "                }\n",
    "                \n",
    "                filename = f'circuit_{circuit_id:04d}_noisy_{noise_type}_{error_rate}.json'\n",
    "                with open(os.path.join(noisy_dir, filename), 'w') as f:\n",
    "                    json.dump(noisy_entry, f, indent=2)\n",
    "            \n",
    "            print(f\"Processed {len(ideal_files)} circuits with {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIntegrator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_training_pairs(self, noisy_dir='data/noisy', output_file='data/qem_dataset.json'):\n",
    "        noisy_files = sorted(Path(noisy_dir).glob('circuit_*_noisy_*.json'))\n",
    "        \n",
    "        print(f\"Creating training dataset from {len(noisy_files)} samples...\")\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for noisy_file in noisy_files:\n",
    "            with open(noisy_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            x_noisy = [list(data['noisy_expectations'].values())[0]]\n",
    "            x_ideal = [list(data['ideal_expectations'].values())[0]]\n",
    "            \n",
    "            training_pair = {\n",
    "                'circuit_id': int(data['circuit_id']),\n",
    "                'num_qubits': int(data['num_qubits']),\n",
    "                'depth': int(data['depth']),\n",
    "                'depth_multiplier': int(data.get('depth_multiplier', 1)),\n",
    "                'entanglement': data.get('entanglement', 'linear'),\n",
    "                'ansatz_type': data.get('ansatz_type', 'EfficientSU2'),\n",
    "                'noise_type': data['noise_type'],\n",
    "                'error_rate': float(data['error_rate']),\n",
    "                'x_noisy': x_noisy,\n",
    "                'x_ideal': x_ideal,\n",
    "                'observable_names': list(data['noisy_expectations'].keys())\n",
    "            }\n",
    "            \n",
    "            training_data.append(training_pair)\n",
    "        \n",
    "        Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(training_data, f, indent=2)\n",
    "        \n",
    "        summary = {\n",
    "            'num_samples': len(training_data),\n",
    "            'num_features': 1,\n",
    "            'ansatz_type': 'EfficientSU2',\n",
    "            'qubit_counts': sorted(list(set(d['num_qubits'] for d in training_data))),\n",
    "            'depth_multipliers': sorted(list(set(d.get('depth_multiplier', 1) for d in training_data))),\n",
    "            'entanglement_types': sorted(list(set(d.get('entanglement', 'linear') for d in training_data))),\n",
    "            'noise_types': sorted(list(set(d['noise_type'] for d in training_data))),\n",
    "            'error_rates': sorted(list(set(d['error_rate'] for d in training_data))),\n",
    "            'observable_used': 'Z⊗Z⊗...⊗Z (global Z measurement)'\n",
    "        }\n",
    "        \n",
    "        summary_file = output_file.replace('.json', '_summary.json')\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"Dataset saved to: {output_file}\")\n",
    "        print(f\"Total samples: {len(training_data)}\")\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def create_combined_dataset(self, qubit_counts, base_output_dir='data'):\n",
    "        print(f\"Combining datasets from all qubit configurations\")\n",
    "        \n",
    "        combined_data = []\n",
    "        qubit_stats = {}\n",
    "        \n",
    "        for num_qubits in qubit_counts:\n",
    "            dataset_file = f'{base_output_dir}/{num_qubits}qubit/qem_dataset_{num_qubits}qubit.json'\n",
    "            \n",
    "            if not os.path.exists(dataset_file):\n",
    "                print(f\"Warning: Dataset for {num_qubits} qubits not found at {dataset_file}\")\n",
    "                continue\n",
    "            \n",
    "            with open(dataset_file, 'r') as f:\n",
    "                qubit_data = json.load(f)\n",
    "            \n",
    "            combined_data.extend(qubit_data)\n",
    "            \n",
    "            qubit_stats[num_qubits] = {\n",
    "                'num_samples': len(qubit_data),\n",
    "                'num_features': 1,\n",
    "            }\n",
    "            \n",
    "            print(f\"Loaded {len(qubit_data)} samples from {num_qubits}-qubit dataset\")\n",
    "        \n",
    "        combined_output_file = f'{base_output_dir}/qem_dataset_combined_full.json'\n",
    "        Path(combined_output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(combined_output_file, 'w') as f:\n",
    "            json.dump(combined_data, f, indent=2)\n",
    "        \n",
    "        combined_summary = {\n",
    "            'total_samples': len(combined_data),\n",
    "            'ansatz_type': 'EfficientSU2',\n",
    "            'qubit_configurations': qubit_counts,\n",
    "            'samples_per_qubit_count': {\n",
    "                str(qc): qubit_stats.get(qc, {}).get('num_samples', 0) \n",
    "                for qc in qubit_counts\n",
    "            },\n",
    "            'features_per_qubit_count': {str(qc): 1 for qc in qubit_counts},\n",
    "            'depth_multipliers': sorted(list(set(d.get('depth_multiplier', 1) for d in combined_data))),\n",
    "            'entanglement_types': sorted(list(set(d.get('entanglement', 'linear') for d in combined_data))),\n",
    "            'noise_types': sorted(list(set(d['noise_type'] for d in combined_data))),\n",
    "            'error_rates': sorted(list(set(d['error_rate'] for d in combined_data))),\n",
    "            'dataset_breakdown': qubit_stats,\n",
    "            'observable_used': 'Z⊗Z⊗...⊗Z (global Z measurement)'\n",
    "        }\n",
    "        \n",
    "        combined_summary_file = f'{base_output_dir}/qem_dataset_combined_full_summary.json'\n",
    "        with open(combined_summary_file, 'w') as f:\n",
    "            json.dump(combined_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"Combined dataset saved to: {combined_output_file}\")\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    def visualize_sample(self, dataset, sample_idx=0):\n",
    "        if not dataset:\n",
    "            print(\"No data to visualize\")\n",
    "            return\n",
    "            \n",
    "        sample = dataset[sample_idx]\n",
    "        print(f\"Sample {sample_idx}:\")\n",
    "        print(f\"  Circuit ID: {sample['circuit_id']}\")\n",
    "        print(f\"  Qubits: {sample['num_qubits']}\")\n",
    "        print(f\"  Depth: {sample['depth']}\")\n",
    "        print(f\"  Noise: {sample['noise_type']} (rate={sample['error_rate']})\")\n",
    "        print(f\"  Noisy expectation: {sample['x_noisy'][0]:.6f}\")\n",
    "        print(f\"  Ideal expectation: {sample['x_ideal'][0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_for_qubit_count(num_qubits, num_circuits=10, base_depth=2, base_output_dir='data'):\n",
    "    print(f\"Generating dataset for {num_qubits} qubits (EfficientSU2)\")\n",
    "    \n",
    "    ideal_dir = f'{base_output_dir}/{num_qubits}qubit/ideal'\n",
    "    noisy_dir = f'{base_output_dir}/{num_qubits}qubit/noisy'\n",
    "    dataset_file = f'{base_output_dir}/{num_qubits}qubit/qem_dataset_{num_qubits}qubit.json'\n",
    "    \n",
    "    depth_multipliers = [1, 2, 3]\n",
    "    entanglement_types = [\"linear\", \"full\", \"pairwise\"]\n",
    "    noise_types = ['depolarizing', 'amplitude_damping', 'readout']\n",
    "    error_rates = [0.001, 0.01, 0.1]\n",
    "    \n",
    "    print(\"Generating ideal circuits...\")\n",
    "    generator = CircuitGenerator(num_qubits=num_qubits)\n",
    "    ideal_dataset = generator.generate_dataset(\n",
    "        num_circuits=num_circuits,\n",
    "        base_depth=base_depth,\n",
    "        depth_multipliers=depth_multipliers,\n",
    "        entanglement_types=entanglement_types,\n",
    "        output_dir=ideal_dir\n",
    "    )\n",
    "    \n",
    "    print(\"Adding noise...\")\n",
    "    noise_injector = NoiseInjector(num_qubits=num_qubits)\n",
    "    noise_injector.add_noise_to_dataset(\n",
    "        ideal_dir=ideal_dir,\n",
    "        noisy_dir=noisy_dir,\n",
    "        noise_types=noise_types,\n",
    "        error_rates=error_rates\n",
    "    )\n",
    "    \n",
    "    print(\"Creating training pairs...\")\n",
    "    integrator = DataIntegrator()\n",
    "    training_data = integrator.create_training_pairs(\n",
    "        noisy_dir=noisy_dir,\n",
    "        output_file=dataset_file\n",
    "    )\n",
    "    \n",
    "    if training_data:\n",
    "        integrator.visualize_sample(training_data, sample_idx=0)\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "def main():\n",
    "    QUBIT_COUNTS = [4, 8]\n",
    "    NUM_CIRCUITS = 10\n",
    "    BASE_DEPTH = 2\n",
    "    BASE_OUTPUT_DIR = 'data'\n",
    "    \n",
    "    all_datasets = {}\n",
    "    \n",
    "    for num_qubits in QUBIT_COUNTS:\n",
    "        try:\n",
    "            training_data = generate_for_qubit_count(\n",
    "                num_qubits=num_qubits,\n",
    "                num_circuits=NUM_CIRCUITS,\n",
    "                base_depth=BASE_DEPTH,\n",
    "                base_output_dir=BASE_OUTPUT_DIR\n",
    "            )\n",
    "            all_datasets[f'{num_qubits}qubit'] = training_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating {num_qubits}-qubit dataset: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"Individual datasets complete!\")\n",
    "    \n",
    "    for num_qubits in QUBIT_COUNTS:\n",
    "        if f'{num_qubits}qubit' in all_datasets:\n",
    "            num_samples = len(all_datasets[f'{num_qubits}qubit'])\n",
    "            expected = NUM_CIRCUITS * 3 * 3 * 3 * 3\n",
    "            \n",
    "            print(f\"\\n{num_qubits} qubits:\")\n",
    "            print(f\"  Generated: {num_samples} samples\")\n",
    "            print(f\"  Expected: {expected} samples\")\n",
    "            print(f\"  Location: data/{num_qubits}qubit/qem_dataset_{num_qubits}qubit.json\")\n",
    "    \n",
    "    integrator = DataIntegrator()\n",
    "    combined_data = integrator.create_combined_dataset(\n",
    "        qubit_counts=QUBIT_COUNTS,\n",
    "        base_output_dir=BASE_OUTPUT_DIR\n",
    "    )\n",
    "    \n",
    "    print(\"Dataset generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b87f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hamim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
